# ðŸŽ¬ Mini-Sora Local AI Video Generation Pipeline  
**Author:** Your Name  
**Version:** 1.0  
**Platform:** macOS (Apple Silicon M1/M2) or Linux (CUDA GPU)  
**Python:** 3.10+

---

## ðŸ§© Overview

**Mini-Sora** is an open-source, fully local text-to-video generation pipeline inspired by OpenAIâ€™s Sora and InVideo AI.  
It produces short cinematic clips by chaining together:

1. **Text â†’ Image** (Stable Diffusion)  
2. **Image â†’ Video** (Stable Video Diffusion)  
3. **Frame Interpolation** (RIFE or FILM)  
4. **Video Refinement** (ffmpeg color grading + upscaling)  
5. **Audio Integration** (ambient, music, or auto-generated voice-over via gTTS)

---

## âš™ï¸ System Requirements

| Component | Minimum | Recommended |
|------------|----------|--------------|
| macOS | 13.0+ | M1/M2 Pro/Max |
| GPU | Apple GPU / NVIDIA 3060+ | NVIDIA 4090 or M2 Max |
| Memory | 16 GB | 32+ GB |
| Disk Space | 20 GB free | 50 GB |
| Python | 3.10+ | 3.11 |

---

## ðŸ§  Dependencies

Install via **venv** or **Pipenv**:

```bash
git clone https://github.com/adills/mini_sora.git
cd mini_sora
python3 -m venv waver_env
source waver_env/bin/activate
# NOTE: On Mac with MPS, you don't need to specify the index-url in the next line
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install diffusers transformers accelerate pillow "imageio[ffmpeg]" gTTS # imageio-ffmpeg for mp4 writing
pip install pytest
brew install ffmpeg

# RIFE
git clone https://github.com/megvii-research/ECCV2022-RIFE rife
cd rife && pip install -r requirements.txt

# FILM
pip install film
```

**Pipenv**
```bash
git clone https://github.com/adills/mini_sora.git
cd mini_sora
pip install pipenv
pipenv --python 3.12
pipenv shell
# NOTE: On Mac with MPS, you don't need to specify the index-url in the next line
pipenv install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pipenv install diffusers transformers accelerate pillow "imageio[ffmpeg]" gTTS # imageio-ffmpeg for mp4 writing
pipenv install --dev pytest
brew install ffmpeg

# RIFE
git clone https://github.com/megvii-research/ECCV2022-RIFE rife
cd rife && pip install -r requirements.txt

# FILM
pipenv install film
```

### Optional (for interpretation)
#### RIFE
git clone https://github.com/megvii-research/ECCV2022-RIFE rife
cd rife && pip install -r requirements.txt

#### FILM
pip install film

## ðŸ“¦ Model downloads / offline use
- First run will download `runwayml/stable-diffusion-v1-5` (textâ†’image) and `stabilityai/stable-video-diffusion-img2vid-xt-1-1` (imageâ†’video). If either is gated, run `hf auth login` (uses a token from https://huggingface.co/settings/tokens) and accept the license.
- To run fully offline, download once and point the env vars to the local folders:
  - `hf download stabilityai/stable-video-diffusion-img2vid-xt-1-1 --local-dir ./models/svd`
  - `export MINI_SORA_VIDEO_MODEL=./models/svd` (old name `MINI_SORA_WAVER_MODEL` still works)
  - (optional) `export MINI_SORA_SD_MODEL=./models/stable-diffusion-v1-5`
- For a quick smoke test without downloads, set `MINI_SORA_TEST_MODE=1` to stub out the heavy stages.

## Usage
Run tests:
```bash
pytest -s tests/test_pipeline_e2e.py
```
The -s flag lets you see printed status lines such as:
```bash
ðŸŽ¨ Generating initial image...
ðŸŽ¥ Generating motion video...
âœ… E2E voice-over test completed.
Final output: /tmp/pytest-.../final_with_voice.mp4
```

Run the main pipeline interactively:

```bash
python mini_sora.py
```

### Input during run
Youâ€™ll be prompted to:
	1.	Choose an interpolation method (RIFE / FILM / none).
	2.	Select audio option: Ambient / Music / Auto Voice-over / None.
	3.	Optionally enter voice-over text and language code.

```bash
Choose interpolation method (RIFE / FILM / none): none
Audio options:
  1 = Ambient
  2 = Music
  3 = Auto Voice-over (gTTS)
  0 = None
Select audio option: 3
Enter your voice-over text (or press Enter for default): A peaceful morning by the lake.
Enter voice language code (default 'en'): en
```

Note: Stable Video Diffusion is image-conditioned, so the â€œmotion promptâ€ text is ignored in the current default video model.

### Memory tips
- If you hit out-of-memory or large buffer errors, try `MINI_SORA_LOW_MEMORY=1` (uses smaller resolution/frames) or override `MINI_SORA_SVD_FRAMES=6 MINI_SORA_SVD_WIDTH=512 MINI_SORA_SVD_HEIGHT=288`.
- Lower decode chunking if needed: `MINI_SORA_SVD_DECODE_CHUNK=3`.
- To force CPU instead of MPS/GPU (very slow, but safer for memory): `MINI_SORA_DEVICE=cpu`.
- To bypass the Stable Diffusion safety checker (e.g., if you keep getting black images), set `MINI_SORA_DISABLE_SAFETY=1`.

**Example Mac MPS minimum low memory CLI**
```bash
MINI_SORA_DEVICE=mps \
MINI_SORA_LOW_MEMORY=1 \
MINI_SORA_DISABLE_SAFETY=1 \
MINI_SORA_SVD_FRAMES=4 \
MINI_SORA_SVD_STEPS=8 \
MINI_SORA_SVD_WIDTH=320 \
MINI_SORA_SVD_HEIGHT=180 \
MINI_SORA_SVD_DECODE_CHUNK=1 \
python3 mini_sora.py
```

**Example of a mixed MPS and CPU process**
```bash
MINI_SORA_IMAGE_DEVICE=mps \
MINI_SORA_VIDEO_DEVICE=cpu \
MINI_SORA_LOW_MEMORY=1 \
MINI_SORA_DISABLE_SAFETY=1 \
MINI_SORA_SVD_FRAMES=8 \
MINI_SORA_SVD_STEPS=16 \
MINI_SORA_SVD_WIDTH=256 \
MINI_SORA_SVD_HEIGHT=448 \
MINI_SORA_SVD_DECODE_CHUNK=3 \
python3 mini_sora.py
```

### Output
âœ… Voice-over saved: audio/voice.wav
âœ… Audio-integrated video ready: outputs/final_with_voice.mp4
ðŸŽ¬ Done! Final video saved as: outputs/final_with_voice.mp4

The final video is saved under:
```outputs/final_with_audio.mp4```

## ðŸŽ™ï¸ Supported Voice Languages (gTTS)
| Code  | Description |
|:-----:|:------------|
| en    | English (US) |
| en-uk | English (UK) |
| en-au | English (Australia) |
| fr    | French |
| es    | Spanish |
| ja    | Japanese |
| hi    | Hindi |
| zh-cn | Chinese (Simplified) |


## ðŸ§© Extensible Modules
| Stage         | Function                                    | File         |
|:--------------|:--------------------------------------------|:-------------|
| Text â†’ Image  | generate_image()                            | mini_sora.py |
| Image â†’ Video | generate_video()                            | mini_sora.py |
| Interpolation | interpolate_frames()                        | mini_sora.py |
| Refinement    | refine_video()                              | mini_sora.py |
| Audio / Voice | add_audio_to_video() / generate_voiceover() | mini_sora.py |

## ðŸ§  Notes for Developers
- Designed for modular import into a Django/Flask backend if needed.
- Each stage returns a file path and can be orchestrated via an external API.
- You can disable any module via flags in the main workflow.

## âœ… Future Enhancements
- Bark / Coqui-TTS integration for offline neural voice synthesis
- Audio beat synchronization using librosa
- Video stabilization for handheld-like motion
- REST API layer for external orchestration

## ðŸ§© License
MIT License Â© 2025 â€” Attribution required.

## ðŸ§ª Example Output
> A 5-second cinematic clip of a woman by a lake, generated fully on-device.
> Output file: outputs/final_with_voice.mp4

## File structure
```text
mini-sora/
â”œâ”€â”€ mini_sora.py                # main script
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_pipeline_unit.py   # unit tests
â”‚   â””â”€â”€ test_pipeline_e2e.py    # full end-to-end test
â”œâ”€â”€ outputs/                    # generated images/videos
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ ambient.wav
â”‚   â”œâ”€â”€ music.mp3
â”‚   â””â”€â”€ voice.wav (optional)
â””â”€â”€ MINI_SORA_PIPELINE.md       # this documentation
```
